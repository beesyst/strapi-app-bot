# Strapi-Proj-Bot

**Strapi-Proj-Bot** — модульная платформа для сбора, агрегации и структурирования данных о крипто-проектах, с поддержкой автоматического парсинга сайтов, X (Twitter), сервисов коллекций типа linktr.ee и очистки данных. Позволяет централизованно управлять настройками, генерировать отчеты, масштабировать сбор и анализировать любые проекты.

## Основные возможности

* **Модульная архитектура** — плагины для сайтов, X-профилей, коллекционных сервисов.
* **Гибкая обработка ссылок** — парсинг из био, коллекционных страниц, генерация и автоматическая нормализация (очистка хвостов, единый формат для YouTube, docs, GitHub и др.).
* **Централизованная настройка** — все параметры и добавление новых проектов в одном `config.json`.
* **Мультиязычность** — легко добавлять новые языки интерфейса.
* **Полная автоматизация** — запуск одной командой, без ручного вмешательства.
* **Обход защит сайтов** — автоматический переход на браузерный режим при Cloudflare, JS-challenge, антиботах.
* **Асинхронная и быстрая обработка** — все этапы пайплайна работают параллельно.
* **Кэширование данных** — минимизация повторных запросов, ускорение парсинга.
* **Логирование** — подробный лог всех шагов для отладки и аудита.

## Где можно использовать

* **Агрегация и мониторинг крипто- и IT-проектов**
* **Автоматизация сбора контактных данных**
* **Обновление витрин и агрегаторов проектов**
* **Парсинг публичных профилей и документации**

## Технологический стек

* **Python** — основной язык разработки
* **Requests, BeautifulSoup** — парсинг и обработка сайтов
* **Playwright** — парсинг X-профилей (через fingerprint)

### Поддерживаемые источники

| Источник          | Описание                                  |
|-------------------|-------------------------------------------|
| `website`         | Главный сайт проекта                      |
| `docs`            | Документация, whitepaper                  |
| `X/Twitter`       | Bio и линки профиля, аватар               |
| `linktr.ee`/etc.  | Сбор всех связанных соцсетей              |
| `YouTube`         | Корректная агрегация только каналов       |
| `GitHub`          | Поддержка фильтрации только по org/user   |

## Архитектура

### Компоненты системы

1. **Парсеры (`core/*.py`)** — обертки над разными источниками (сайты, коллекционные сервисы, X/Twitter).
2. **Центральная точка входа (`config/start.py`)** — управляет пайплайном сбора, нормализации и сохранения данных.
3. **Шаблоны (`templates/`)** — структуруют результат под формат системы.
4. **Логирование (`logs/`)** — ведет полный журнал работы.
5. **Конфигурация (`config/config.json`)** — все цели, настройки и параметры.

### Структура проекта

```
strapi-proj-bot/
├── config/
│   ├── apps/
│   │   └── {project}.json         # Конфиг отдельного приложения
│   ├── config.json                # Центральная конфигурация (все проекты, параметры)
│   └── start.py                   # Главный скрипт пайплайна (точка входа)
├── core/
│   ├── api_ai.py                  # Интеграция с AI
│   ├── api_strapi.py              # Интеграция с API Strapi
│   ├── api_coingecko.py           # Интеграция с API Coingecko
│   ├── browser_fetch.js           # Парсер сайтов через браузер
│   ├── install.py                 # Скрипт автоустановки зависимостей
│   ├── log_utils.py               # Логирование
│   ├── orchestrator.py            # Оркестрация (main async pipeline)
│   ├── package.json               # Зависимости парсеров (Node)
│   ├── seo_utils.py               # Заполнение SEO
│   ├── status.py                  # Статусы
│   ├── package-lock.json          # Лок-файл зависимостей
│   ├── twitter_parser.js          # Парсер X профилей (Node)
│   └── web_parser.py              # Модуль парсинга ссылок
├── logs/
│   ├── ai.log                     # Лог AI
│   ├── host.log                   # Хостовой лог пайплайна
│   ├── setup.log                  # Лог установки зависимостей
│   └── strapi.log                 # Лог отправки в Strapi
├── storage/
│   └── apps/
│       └── {project}/
│           └── main.json          # Результаты парсинга по проекту
├── templates/
│   └── main_template.json         # Шаблон структуры main.json
├── requirements.txt               # Python зависимости
├── README.md                      # Документация
└── start.sh                       # Bash-скрипт быстрого запуска пайплайна
```

## Pipeline: Как это работает?

1. **Запуск системы**:
   * `start.sh` → `config/start.py` → `core/orchestrator.py`
2. **Автоустановка зависимостей**:
   * `config/start.py` →`core/install.py`:
      * Устанавливаются все Python-пакеты (requirements.txt).
      * Устанавливаются Node.js-модули (для антибот-парсинга и Twitter/X).
      * Playwright автоматически загружает браузеры для headless-парсинга.
3. **Загрузка конфигурации и шаблонов**:
   * Загружается основной конфиг (`config/config.json`): список целей, настройки, категории, ключи API.
   * Подгружается шаблон данных `templates/main_template.json` (структура и ключи main.json).
4. **Асинхронный парсинг и сбор данных для каждой цели**:
   * **Быстрый web-парсинг:** через `requests` + `BeautifulSoup` для большинства сайтов.
   * **Обход защиты (Cloudflare, JS, антибот):** при обнаружении защиты автоматический переход на `Playwright` + Fingerprint Suite (`core/browser_fetch.js`).
   * **Twitter/X:** всегда отдельный браузерный парсер (`core/twitter_parser.js`) с реальным поведением.
   * **Обработка docs, коллекционных и внутренних ссылок:** (linktr.ee, read.cv и др.) — либо requests, либо Playwright.
   * **Детект и нормализация всех соц-ссылок и docs:** GitHub, Discord, Telegram, Medium, YouTube, LinkedIn, Reddit, docs и т.д.
   * **Кэширование HTML:** in-memory кэш для ускорения и снижения нагрузки.
   * **Асинхронность и параллелизм:** все процессы по каждому проекту (AI-генерация, CoinGecko, web-парсинг, enrichment) выполняются параллельно (asyncio + ThreadPool).
   * **Ретраи и обработка ошибок:** автоматические повторы при ошибках, логирование каждого шага.
5. **AI-генерация описаний, enrichment и автоматизация категорий**:
   * Автоматический запуск AI-генерации краткого и полного описания.
   * Поиск информации о токене/коине через CoinGecko API (c fallback на ручной шаблон).
   * **Автоматическая генерация релевантных категорий через AI** — далее маппинг в Strapi-ID с созданием недостающих категорий автоматически.
6. **Сохранение результата**:
   * Все данные по каждому проекту сохраняются в `storage/apps/{app}/{project}/main.json` (или storage/total, если используется общий сбор).
7. **Публикация и интеграция**:
   * Готовые `main.json` **автоматически** заливаются в Strapi через API.
   * Картинки/лого автоматически прикрепляются к проекту в Strapi, SEO поля обновляются.

**Запускать нужно только `start.sh` — все остальное сделает бот!**

## Установка и запуск

```bash
git clone https://github.com/beesyst/strapi-proj-bot.git
cd strapi-proj-bot
bash start.sh
```

## Настройка конфигурации
Все параметры задаются в файле config/config.json:

| Параметр   | Значение по умолчанию | Описание                                                     |
|------------|-----------------------|--------------------------------------------------------------|
| `apps`     | `[ "babylon" ]`       | Список целей (объекты-проекты с настройками и enabled)       |
| `enabled`  | `true`                | Флаг: включен ли проект (false — будет полностью проигнорирован) |
| `link_collections` | `[ "linktr.ee" ]` | Массив сервисов для глубокого парсинга                 |

## Терминал и статусы

В процессе работы бот выводит для каждого проекта только итоговый статус:

* `[add]` — проект добавлен впервые (создан новый main.json, отправлен в Strapi)
* `[update]` — данные проекта обновлены (main.json перезаписан, отправлен в Strapi)
* `[skip]` — данные не изменились (ничего не отправлялось)
* `[error]` — возникла ошибка при сборе или отправке

